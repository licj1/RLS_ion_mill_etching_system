{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Editor: @Hyunhomo\n",
    "Reference: https://github.com/TobiasGlaubach/python-ml-turbofan\n",
    "'''\n",
    "\n",
    "## Import libraries in python\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import padasip as pa\n",
    "# pa.filters.FilterRLS(n)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import importlib\n",
    "from scipy.stats import randint, expon, uniform\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "num_cycles = 4000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Data/01_M02_DC_train_ttf_merged.csv' does not exist: b'Data/01_M02_DC_train_ttf_merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d1881a891d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train_ttf_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/01_M02_DC_train_ttf_merged.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train_ttf_merged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Data/01_M02_DC_train_ttf_merged.csv' does not exist: b'Data/01_M02_DC_train_ttf_merged.csv'"
     ]
    }
   ],
   "source": [
    "df_train_ttf_merged = pd.read_csv('Data/01_M02_DC_train_ttf_merged.csv')\n",
    "df_train_ttf_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only count \n",
    "df_train_ttf_merged = df_train_ttf_merged.loc[df_train_ttf_merged['FIXTURESHUTTERPOSITION'] == 1.0]\n",
    "df_train_ttf_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ttf_merged[df_train_ttf_merged['failure_cycle'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load fault data if needed\n",
    "df_01_M02_train_fault_data = pd.read_csv('Data/01_M02_train_fault_data.csv', nrows = num_cycles)\n",
    "# pd.set_option('display.max_rows', 1000)\n",
    "# df_01_M02_train_fault_data\n",
    "# last time row : 25633658\n",
    "\n",
    "time_FPDBL_fault = df_01_M02_train_fault_data['time'].loc[df_01_M02_train_fault_data['fault_name'] == 'FlowCool Pressure Dropped Below Limit']\n",
    "time_FPDBL_fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot first cycle \n",
    "\n",
    "# df_train_ttf_merged = df_train_ttf_merged[df_train_ttf_merged['recipe_step']!=49]\n",
    "\n",
    "\n",
    "# cols_attributes = ['time','IONGAUGEPRESSURE','ETCHBEAMVOLTAGE', 'ETCHBEAMCURRENT', 'ETCHSUPPRESSORCURRENT', 'ETCHGASCHANNEL1READBACK','FLOWCOOLFLOWRATE', 'FLOWCOOLPRESSURE']\n",
    "cols_attributes =  df_train_ttf_merged.columns\n",
    "print (cols_attributes)\n",
    "t = pd.DataFrame(df_train_ttf_merged[df_train_ttf_merged['failure_cycle'] == 1][cols_attributes].values, columns=cols_attributes).plot(x = 'time', subplots=True, figsize=(15, 15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reshape features into (samples, time steps, features)\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "#     print (\"data_matrix\", data_matrix)\n",
    "    print (\" data_matrix.shape\",  data_matrix.shape)\n",
    "#     print (\"data_matrix.shape[0]\", data_matrix.shape[0])\n",
    "    num_elements = data_matrix.shape[0]\n",
    "#     print (\"num_elements\", num_elements)\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements - seq_length - pred_length), range(seq_length, num_elements - pred_length)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n",
    "        \n",
    "def gen_target(id_df, seq_length, pred_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "#     print (\" data_matrix.shape\",  data_matrix.shape)\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "#     print (\"num_elements\", num_elements)\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(seq_length, num_elements  -pred_length), range(seq_length + pred_length , num_elements )):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "        \n",
    "        \n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]]\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements- pred_length, :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define functions for anomaly detection (analysis of prediction errors)\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def EWMA_Calc(epsilon, mu, lmbd_mu):\n",
    "    \"\"\"\n",
    "    epsilon - input value for current time stamp (prediction error)\n",
    "    mu - previous EWMA (average of until previous time stamp)\n",
    "    lmbd_mu - a constant smoothing factor for EWMA weighting decrease\n",
    "    \"\"\"\n",
    "    # calculation formula\n",
    "    ewma = (lmbd_mu * epsilon) + ((1-lmbd_mu)*mu) \n",
    "    # return the result\n",
    "    return ewma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def EWMA_Update(epsilon, anomaly, mu, sigma, lmbd_mu, lmbd_sigma):\n",
    "    \"\"\"\n",
    "    epsilon - input value for current time stamp (prediction error)\n",
    "    anomaly - True(1)/False(0) of anomaly until previous time stamp\n",
    "    mu - previous EWMA (average of until previous time stamp)\n",
    "    sigma - previous absolute deviation\n",
    "    lmbd_mu - a constant smoothing factor for EWMA weighting decrease\n",
    "    lmbd_sigma - a constant smoothing factor for the calculation of updating absolute deviation\n",
    "    \"\"\"\n",
    "    if anomaly == False :\n",
    "        # update absolute deviation(sigma) if there is no anomaly up to now\n",
    "        sigma = EWMA_Calc ( abs(epsilon - mu), sigma , lmbd_sigma )\n",
    "        \n",
    "    # update EWMA \n",
    "    mu = EWMA_Calc( epsilon, mu, lmbd_mu )\n",
    "    sigma = sigma\n",
    "    # return mu and sigma, mu(average is always updated, \n",
    "    # but abs deviation is updated only when anomaly is False)\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def InitPhaseOne (epsilon, mu, sigma, lmbd_mu, lmbd_sigma, anomaly=False):\n",
    "    \"\"\"\n",
    "    'initial phase to calculate/update average for the first few time stamps'\n",
    "    \n",
    "    \"\"\"\n",
    "    mu, sigma = EWMA_Update(epsilon, anomaly, mu, sigma, lmbd_mu, lmbd_sigma)\n",
    "    \n",
    "    return mu, sigma\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "def InitPhaseTwo (epsilon, mu, sigma, lmbd_mu, lmbd_sigma, thold_ad, anomaly=False):\n",
    "    \"\"\"\n",
    "    'second phase of initialization to adapt threshold'\n",
    "    thold_ad - threshold factor for the anomaly detection \n",
    "    \n",
    "    \"\"\"\n",
    "    mu, sigma = EWMA_Update(epsilon, anomaly, mu, sigma, lmbd_mu, lmbd_sigma)    \n",
    "    \n",
    "    if abs(epsilon - mu) > sigma * thold_ad : \n",
    "        thold_ad = abs(epsilon - mu) / sigma\n",
    "    \n",
    "    \n",
    "    return mu, sigma , thold_ad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def InitPhasePvalue (epsilon, mu, sigma, thold_p, conf_lv = 0.05):\n",
    "    \"\"\"\n",
    "    'induce thold_p value to evaluate p-value in detection step'\n",
    "    thold_p - threshold factor for the p-value\n",
    "    conf_lv - user defined confidence level, 0.05 means 95%\n",
    "    \n",
    "    \"\"\"       \n",
    "    fixed_thold = stats.norm.ppf(conf_lv, loc = mu, scale = thold_p*sigma)\n",
    "    deviation_thold = abs(fixed_thold - mu)\n",
    "    \n",
    "    # loop until thold_p shows can be enough variance in normal distribution    \n",
    "    while abs(epsilon-mu)> deviation_thold :\n",
    "        thold_p += 0.1        \n",
    "        fixed_thold = stats.norm.ppf(conf_lv, loc = mu, scale = thold_p*sigma)\n",
    "        deviation_thold = abs(fixed_thold - mu)\n",
    "        \n",
    "#         print (\"bad loop- thold_p:%s, sigma:%s\" %(thold_p,sigma))\n",
    "        \n",
    "    \n",
    "    return thold_p\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def Detection (epsilon, mu, sigma, lmbd_mu, lmbd_sigma, thold_ad, thold_p, anomaly=False):\n",
    "    \n",
    "    '''\n",
    "    main function which detect anomaly and calculate p-value\n",
    "    epsilon - current inpur prediction error\n",
    "    anomaly - default is False, the detection result (True/False) will be return\n",
    "    mu, sigma - updated average or deviation value during initalization phase\n",
    "    lmbd_mu, lmbd_sigma - manually selected smoothing factor for EWMA calculation\n",
    "    thold_ad, thold_p - adaptive threshold factor value, initial value is determined in inital phase\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if abs(epsilon - mu) > sigma * thold_ad : \n",
    "        anomaly = True\n",
    "    \n",
    "    \n",
    "    mu, sigma = EWMA_Update(epsilon, anomaly, mu, sigma, lmbd_mu, lmbd_sigma)\n",
    "    \n",
    "    # half tailed p-value (only concerning deviation from the mean, range:[0,0.5])\n",
    "    # if p_value is supposed to be used for anomaly detection, \n",
    "    # then anomaly is true when p_value is larger than conf_lv(user defined confidence level)  \n",
    "    if epsilon > mu :\n",
    "        p_value = stats.norm.sf(epsilon, mu, thold_p*sigma) #p-value right tailed (right hand side of mean)\n",
    "    else :\n",
    "        p_value = stats.norm.cdf(epsilon, mu, thold_p*sigma) #p-value left tailed (left hand side of mean)\n",
    "#     print (p_value)\n",
    "\n",
    "    \n",
    "    return mu, sigma , anomaly, p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    " data_matrix.shape (1291, 11)\n",
    " data_matrix.shape (2774, 11)\n",
    " data_matrix.shape (1055, 11)\n",
    " data_matrix.shape (47, 11)\n",
    " data_matrix.shape (560407, 11)\n",
    " data_matrix.shape (174, 11)\n",
    " data_matrix.shape (73, 11)\n",
    " data_matrix.shape (38, 11)\n",
    " data_matrix.shape (16, 11)\n",
    " data_matrix.shape (61250, 11)\n",
    " data_matrix.shape (1029592, 11)\n",
    " data_matrix.shape (7507, 11)\n",
    " data_matrix.shape (306920, 11)\n",
    " data_matrix.shape (58763, 11)\n",
    " data_matrix.shape (37463, 11)\n",
    " data_matrix.shape (106375, 11)\n",
    " data_matrix.shape (56916, 11)\n",
    " data_matrix.shape (4421, 11)\n",
    " data_matrix.shape (12266, 11)\n",
    " data_matrix.shape (70580, 11)\n",
    " data_matrix.shape (613419, 11)\n",
    " data_matrix.shape (398, 11)\n",
    "\n",
    "\n",
    " data_matrix.shape (560407, 11)\n",
    " data_matrix.shape (61250, 11)\n",
    " data_matrix.shape (1029592, 11)\n",
    " data_matrix.shape (7507, 11)\n",
    " data_matrix.shape (306920, 11)\n",
    " data_matrix.shape (58763, 11)\n",
    " data_matrix.shape (37463, 11)\n",
    " data_matrix.shape (106375, 11)\n",
    " data_matrix.shape (56916, 11)\n",
    " data_matrix.shape (4421, 11)\n",
    " data_matrix.shape (12266, 11)\n",
    " data_matrix.shape (70580, 11)\n",
    "\n",
    " \n",
    "'''\n",
    "\n",
    "# sequence_length = 100\n",
    "sequence_length = 10\n",
    "pred_length = 1\n",
    "lr = 0.99\n",
    "# cycle_idx = 96 # 1~100\n",
    "# sensor_idx = 14  # 0~13, sensor 2,3,4,7,8,9,11,12,13,14,15,17,20 and 21 in order\n",
    "\n",
    "# RMSE of first n samples and last n samples\n",
    "n = 1000\n",
    "\n",
    "cycle_list = df_train_ttf_merged['failure_cycle'].unique()\n",
    "print (cycle_list)\n",
    "\n",
    "# cycle_list = cycle_list[[ 0, 5, 10, 12, 13, 14, 16, 17, 18, 19]]\n",
    "\n",
    "cycle_list = cycle_list[[ 0, 10,13, 16, 17]]\n",
    "\n",
    "print (len(cycle_list))\n",
    "print (cycle_list)\n",
    "\n",
    "\n",
    "p_err_u_list = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Initialize the parameters for anomaly detection \n",
    "anomaly = False\n",
    "mu = 10\n",
    "sigma = 10\n",
    "lmbd_mu = 0.1\n",
    "lmbd_sigma = 0.1\n",
    "thold_ad = 0.5\n",
    "thold_p = 0.5\n",
    "\n",
    "'''\n",
    "Index(['time', 'Tool', 'stage', 'Lot', 'runnum', 'recipe', 'recipe_step',\n",
    "       'IONGAUGEPRESSURE', 'ETCHBEAMVOLTAGE', 'ETCHBEAMCURRENT',\n",
    "       'ETCHSUPPRESSORVOLTAGE', 'ETCHSUPPRESSORCURRENT', 'FLOWCOOLFLOWRATE',\n",
    "       'FLOWCOOLPRESSURE', 'ETCHGASCHANNEL1READBACK', 'ETCHPBNGASREADBACK',\n",
    "       'FIXTURETILTANGLE', 'ROTATIONSPEED', 'ACTUALROTATIONANGLE',\n",
    "       'FIXTURESHUTTERPOSITION', 'ETCHSOURCEUSAGE', 'ETCHAUXSOURCETIMER',\n",
    "       'ETCHAUX2SOURCETIMER', 'ACTUALSTEPDURATION',\n",
    "       'TTF_FlowCool Pressure Dropped Below Limit',\n",
    "       'TTF_Flowcool Pressure Too High Check Flowcool Pump',\n",
    "       'TTF_Flowcool leak', 'failure_cycle'],\n",
    "      dtype='object')\n",
    "\n",
    "\n",
    "'''\n",
    "sensor_idx = ['IONGAUGEPRESSURE','ETCHBEAMVOLTAGE', 'ETCHBEAMCURRENT', 'ETCHSUPPRESSORVOLTAGE',\n",
    "              'ETCHSUPPRESSORCURRENT', 'FLOWCOOLFLOWRATE','FLOWCOOLPRESSURE',\n",
    "              'ETCHGASCHANNEL1READBACK', 'ETCHPBNGASREADBACK',\n",
    "              'FIXTURETILTANGLE','ACTUALSTEPDURATION']\n",
    "\n",
    "\n",
    "\n",
    "for cycle in cycle_list:\n",
    "    \n",
    "    p_err_s_list = []\n",
    "    \n",
    "    # pick the feature columns\n",
    "    sensor_dataframe = df_train_ttf_merged.drop(columns=['time','Tool', 'stage' , 'runnum', 'Lot','recipe','recipe_step',\n",
    "                                                         'ROTATIONSPEED', 'ACTUALROTATIONANGLE',\n",
    "                                                          'ETCHSOURCEUSAGE', 'ETCHAUXSOURCETIMER','ETCHAUX2SOURCETIMER',\n",
    "                                                         'FIXTURESHUTTERPOSITION','TTF_FlowCool Pressure Dropped Below Limit',\n",
    "                                                         'TTF_Flowcool Pressure Too High Check Flowcool Pump','TTF_Flowcool leak',\n",
    "                                                        'failure_cycle'])\n",
    "    sequence_cols = sensor_dataframe.columns.values.tolist()\n",
    "    print (\"sequence_cols\",sequence_cols)\n",
    "\n",
    "    # generator for the sequences\n",
    "    # transform each id of the train dataset in a sequence\n",
    "    seq_gen = (list(gen_sequence(df_train_ttf_merged[df_train_ttf_merged['failure_cycle'] == id], sequence_length, sequence_cols))\n",
    "               for id in np.delete(cycle_list,np.where(cycle_list == cycle)))\n",
    "\n",
    "    # generate sequences and convert to numpy array\n",
    "    print (\"sequence generated, concatenating...\")\n",
    "    seq_array  = np.concatenate(list(seq_gen))\n",
    "    # seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "#     print(seq_array)\n",
    "\n",
    "    print (\"concatenated!\")\n",
    "\n",
    "    # generator for the sequences\n",
    "    # transform each id of the train dataset in a sequence\n",
    "    target_seq_gen = (list(gen_target(df_train_ttf_merged[df_train_ttf_merged['failure_cycle'] == id], sequence_length, pred_length, sequence_cols))\n",
    "               for id in np.delete(cycle_list,np.where(cycle_list == cycle)))\n",
    "\n",
    "    # print (\"target_seq_gen\", target_seq_gen)\n",
    "    # generate sequences and convert to numpy array\n",
    "    target_seq_array  = np.concatenate(list(target_seq_gen))\n",
    "    # target_seq_array = np.concatenate(list(target_seq_gen)).astype(np.float32)\n",
    "#     print(target_seq_array)\n",
    "#     print(target_seq_array.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # generate RUL labels\n",
    "    label_gen = [gen_labels(df_train_ttf_merged[df_train_ttf_merged['failure_cycle'] == id], \n",
    "                            sequence_length, ['TTF_FlowCool Pressure Dropped Below Limit'])\n",
    "                 for id in np.delete(cycle_list,np.where(cycle_list == cycle))]\n",
    "\n",
    "    label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "    \n",
    "    ## sequence of time series data and target measurement values for selected machine (unit_id_idx)\n",
    "    seq_gen_m_id = (list(gen_sequence(df_train_ttf_merged[df_train_ttf_merged['failure_cycle'] == cycle], sequence_length, sequence_cols))\n",
    "               )\n",
    "\n",
    "    # generate sequences and convert to numpy array\n",
    "    seq_array_m_id  = np.array(list(seq_gen_m_id))\n",
    "    # seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "    # print(seq_array_m_id)\n",
    "#     print(seq_array_m_id.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # generator for the sequences\n",
    "    # transform each id of the train dataset in a sequence\n",
    "    target_seq_gen_m_id = (list(gen_target(df_train_ttf_merged[df_train_ttf_merged['failure_cycle'] == cycle], sequence_length, pred_length, sequence_cols))\n",
    "               )\n",
    "\n",
    "    # print (\"target_seq_gen\", target_seq_gen)\n",
    "    # generate sequences and convert to numpy array\n",
    "    target_seq_array_m_id  = np.array(list(target_seq_gen_m_id))\n",
    "    # target_seq_array = np.concatenate(list(target_seq_gen)).astype(np.float32)\n",
    "    # print(target_seq_array_m_id)\n",
    "#     print(target_seq_array_m_id.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for s_i in range(len(sensor_idx)):\n",
    "        print (\"sensor loop %s: %s\" %(s_i,sensor_idx[s_i] ))\n",
    "        \n",
    "        # Initialize the parameters of anomaly detection for each time series (sensor data)\n",
    "        anomaly = False\n",
    "        mu = 10\n",
    "        sigma = 10\n",
    "        lmbd_mu = 0.1\n",
    "        lmbd_sigma = 0.1\n",
    "        thold_ad = 0.5\n",
    "        thold_p = 0.5\n",
    "\n",
    "\n",
    "        ### Prediction error estimation with RLS\n",
    "\n",
    "        ## Check data shape (last instance of each unit is excluded)\n",
    "    #     print (\"other sensors input data shape: \" , seq_array[:,:,0].shape)\n",
    "    #     print (\"other sensors target measurement shape: \", target_seq_array[:,:,0].shape)\n",
    "    #     print (\"RUL array shape: \", label_array.shape)\n",
    "\n",
    "        ## Apadt model first\n",
    "        ## An example how to filter data measured in real-time\n",
    "        # these two function supplement your online measurment\n",
    "\n",
    "        s2_input_array = seq_array[:,:,s_i]\n",
    "        s2_target_array = target_seq_array[:,:,s_i]\n",
    "\n",
    "        N = s2_input_array.shape[0]\n",
    "        log_d = np.zeros(N)\n",
    "        log_y = np.zeros(N)\n",
    "        filt = pa.filters.FilterRLS(sequence_length, mu=lr)\n",
    "        \n",
    "        init_p_err_list = []\n",
    "        \n",
    "        print (\"RLS for adaptation(other cycles)\")\n",
    "        for k in tqdm(range(N-1)):\n",
    "            # measure input\n",
    "#             print (\"k\",k)\n",
    "            x = s2_input_array[k]\n",
    "        #     print (x)\n",
    "        #     print (x.shape)\n",
    "            # predict new value\n",
    "            y = filt.predict(x)\n",
    "            # do the important stuff with prediction output\n",
    "#             pass    \n",
    "            # measure output\n",
    "            d = s2_target_array[k]\n",
    "        #     print (\"d\", d)\n",
    "        #     print (\"y\", y)\n",
    "            # update filter\n",
    "            filt.adapt(d, x)\n",
    "\n",
    "            epsilon = d-y\n",
    "            \"\"\"\n",
    "            Here, after computing prediction error for each sensor of other TTF(R2F) cycle, use those\n",
    "            prediction erros to initinalize the parameters(factors) for anomaly detection \n",
    "\n",
    "            \"\"\"\n",
    "            #initialize mean and absolute deviation\n",
    "            \n",
    "            if k < 10 :\n",
    "                pass\n",
    "                \n",
    "            elif k >= 10 and k < 30 :\n",
    "                init_p_err_list.append(epsilon)\n",
    "#                 print (init_p_err_list)\n",
    "            \n",
    "            # start from 'mean of first 10 samples', 'absolute deviation of a sample from this mean'\n",
    "            elif k == 30:\n",
    "#                 print (\"it should be appear only 30\")\n",
    "                mu = np.mean(init_p_err_list)\n",
    "                sigma = abs(init_p_err_list[int(k/2)] - mu)\n",
    "            \n",
    "            # half of overall sample is used for the first initial phase\n",
    "            elif k > 30 and k < int((N-1)/2) :\n",
    "                mu, sigma = InitPhaseOne (epsilon, mu, sigma, lmbd_mu, lmbd_sigma)\n",
    "            \n",
    "            # remaining half would be used for the second phase\n",
    "            else :\n",
    "#                 pass\n",
    "#                 print (\"it is else statement\")\n",
    "                mu,sigma, thold_ad = InitPhaseTwo (epsilon, mu, sigma, lmbd_mu, lmbd_sigma, thold_ad)\n",
    "                thold_p = InitPhasePvalue (epsilon, mu, sigma, thold_p, conf_lv = 0.20)\n",
    "#             print (\"epsilon:%s, mu:%s, signma:%s, thold_ad:%s, thold_p:%s\" %(epsilon, mu,sigma, thold_ad, thold_p))\n",
    "\n",
    "        print (\"Initialized value of sensor %s -  mu:%s, signma:%s, thold_ad:%s, thold_p:%s\" %(s_i, mu,sigma, thold_ad, thold_p))    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        #only sensor_02\n",
    "\n",
    "        s2_input_m_id = seq_array_m_id[:,:,s_i]\n",
    "        s2_target_m_id = target_seq_array_m_id[:,:,s_i]\n",
    "\n",
    "\n",
    "        N = s2_input_m_id.shape[0]\n",
    "        log_d = np.zeros(N-1)\n",
    "        log_y = np.zeros(N-1)\n",
    "        epsilon_array = np.zeros(N-1)\n",
    "        anomaly_array = np.zeros(N-1)\n",
    "        p_value_array = np.zeros(N-1)\n",
    "        \n",
    "        print (\"RLS for selected sensor\")\n",
    "        for k in tqdm(range(N-1)):\n",
    "            # measure input\n",
    "#             print (\"k\",k)\n",
    "            x = s2_input_m_id[k]\n",
    "        #     print (x)\n",
    "        #     print (x.shape)\n",
    "            # predict new value\n",
    "            y = filt.predict(x)\n",
    "            # do the important stuff with prediction output\n",
    "#             pass    \n",
    "            # measure output\n",
    "            d = s2_target_m_id[k]\n",
    "    #         print (\"d\", d)\n",
    "    #         print (\"y\", y)\n",
    "            # update filter\n",
    "            filt.adapt(d, x)\n",
    "            # log values\n",
    "            log_d[k] = d\n",
    "            log_y[k] = y\n",
    "\n",
    "            epsilon = d-y\n",
    "            \"\"\"\n",
    "            Use Initilized paramters above to compute anomaly and p value,\n",
    "            compute every loop( online)\n",
    "            anomaly(True/False) and p-value will be assigned into array/list and display\n",
    "            \n",
    "            \"\"\"\n",
    "            mu, sigma , anomaly, p_value = Detection (epsilon, mu, sigma, lmbd_mu, lmbd_sigma, thold_ad, thold_p, anomaly=False)\n",
    "#             print (\"online update - epsilon:%s,  mu:%s , sigma:%s\" %(epsilon, mu,sigma))\n",
    "#             print (\"detection results - anaomaly:%s , p_value:%s\" %(anomaly, p_value))\n",
    "            \n",
    "            # log values\n",
    "            log_d[k] = d\n",
    "            log_y[k] = y\n",
    "            # save prediction error to array for each sensor\n",
    "            epsilon_array[k] = epsilon\n",
    "            anomaly_array[k] = anomaly\n",
    "            p_value_array[k] = p_value\n",
    "            \n",
    "        \n",
    "        # An array of each sensor is appended to list\n",
    "        p_err_s_list.append(epsilon_array)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        ### show results\n",
    "        plt.figure(figsize=(15,9))\n",
    "        plt.subplot(311);plt.title(\"Adaptation for unit:%s sensor:%s length:%s lr:%s\" %(cycle,s_i, sequence_length, lr));plt.xlabel(\"time unit(cycle)\")\n",
    "        plt.plot(log_d,\"b\", label=\"d - target\")\n",
    "        plt.plot(log_y,\"g\", label=\"y - output\");plt.legend()\n",
    "        plt.subplot(312);plt.title(\"Adaptation for unit:%s sensor:%s length:%s lr:%s\" %(cycle,s_i, sequence_length, lr));plt.xlabel(\"time unit(cycle)\")\n",
    "        plt.plot(log_d,\"b\", label=\"d - target\");plt.legend()\n",
    "        ax1 = plt.subplot(313)\n",
    "        ax1.set_xlabel(\"time unit(cycle)\")\n",
    "        ax2 = ax1.twinx()\n",
    "        ax1.plot(10*np.log10((log_d-log_y)**2),\"r\", label=\"e - error [dB]\");ax1.legend()\n",
    "        ax2.plot(anomaly_array,\"b\", label=\"anomaly detection\");ax2.legend()\n",
    "        plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## Plots for the anomaly detection\n",
    "        plt.figure(figsize=(15,9))\n",
    "        ax1 = plt.subplot(211);plt.title(\"p-value of prediction error for unit:%s sensor:%s \" %(cycle,s_i));plt.xlabel(\"time unit(cycle)\")\n",
    "        ax1.set_xlabel(\"time unit(cycle)\")\n",
    "        ax2 = ax1.twinx()\n",
    "        ax1.plot(10*np.log10((log_d-log_y)**2),\"r\", label=\"e - error [dB]\");ax1.legend()      \n",
    "        ax2.plot(p_value_array,\"darkgoldenrod\", label=\"p-value\");ax2.legend()        \n",
    "        plt.subplot(212);plt.title(\"The result of anomaly detection for unit:%s sensor:%s \" %(cycle,s_i));plt.xlabel(\"time unit(cycle)\")\n",
    "        plt.plot(anomaly_array,\"b\", label=\"anomaly detection\");plt.legend()\n",
    "        plt.legend(); plt.tight_layout(); plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "        ## Numerical evaluation\n",
    "\n",
    "\n",
    "        y_actual = log_d\n",
    "        y_predicted = log_y\n",
    "\n",
    "        first_n_rmse = sqrt(mean_squared_error(y_actual[:n], y_predicted[:n]))\n",
    "        last_n_rmse = sqrt(mean_squared_error(y_actual[-n:], y_predicted[-n:]))\n",
    "\n",
    "    #     print (\"first %s cycles RMSE of prediction: %s\" %(n,first_n_rmse))\n",
    "    #     print (\"last %s cycles RMSE of prediction: %s\" %(n,last_n_rmse))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        RMSE evaluation with fixed window size n\n",
    "        '''\n",
    "\n",
    "        # s2_input_m_id = seq_array_m_id[:,:,sensor_idx]\n",
    "        # s2_target_m_id = target_seq_array_m_id[:,:,sensor_idx]\n",
    "        # x = s2_input_m_id\n",
    "        # d = s2_target_m_id\n",
    "\n",
    "        # y, e, w = filt.run(d, x)\n",
    "        # y_actual = d\n",
    "        # y_predicted = y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     print (len(y_predicted))\n",
    "    #     print (int(len(y_predicted)/n))\n",
    "        rmse_list = []\n",
    "        for iter in range(int(len(y_predicted)/n)) :\n",
    "            rmse_temp = sqrt(mean_squared_error(y_actual[n*iter:n*(iter+1)], y_predicted[n*iter:n*(iter+1)]))\n",
    "            rmse_list.append(rmse_temp)\n",
    "\n",
    "\n",
    "\n",
    "    #     print (rmse_list)\n",
    "\n",
    "        ### show results\n",
    "        fig_rmse = plt.figure(figsize=(15,5))\n",
    "        plt.plot(rmse_list,\"b\", label=\"RMSE\")\n",
    "        plt.xlabel(\"cycles/%s\" %n)\n",
    "        plt.title(\"RMSE for every %s cycles for unit:%s sensor:%s length:%s lr:%s\" %(n,cycle,s_i, sequence_length, lr))\n",
    "        plt.legend(); plt.tight_layout(); plt.show()\n",
    "        fig_rmse.savefig(\"plots/rmse_u_%s_s_%s_len_%s_lr_%s.png\" %(cycle,s_i,sequence_length, lr))\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        filt = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
